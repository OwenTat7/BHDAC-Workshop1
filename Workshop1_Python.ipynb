{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing with Python\n",
    "\n",
    "This notebook will guide you through data cleaning and preprocessing steps using the Netflix dataset. We'll cover concepts such as handling missing data, detecting outliers, renaming columns, dealing with inconsistent data, feature engineering, data transformation, encoding categorical variables, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### About this Dataset: [Netflix Movies & TV Shows](https://www.kaggle.com/datasets/shivamb/netflix-shows)\n",
    "\n",
    "Netflix is one of the most popular media and video streaming platforms. They have over 8000 movies or tv shows available on their platform, as of mid-2021, they have over 200M Subscribers globally. This tabular dataset consists of listings of all the movies and tv shows available on Netflix, along with details such as - cast, directors, ratings, release year, duration, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Due Diligence: Initial Data Exploration\n",
    "\n",
    "To begin, let's load the dataset and perform an initial exploration to understand its structure and contents.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/path/to/netflix_titles.csv')\n",
    "\n",
    "# Inspecting the data\n",
    "df.dtypes\n",
    "df.head()\n",
    "df.tail()\n",
    "df.info()\n",
    "df.describe().T\n",
    "df.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv('netflix_titles.csv')\n",
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Data\n",
    "We'll drop unnecessary columns and handle any remaining missing values.\n",
    "\n",
    "```python \n",
    "# Dropping columns\n",
    "df = df.drop(['director', 'cast', 'country', 'date_added', 'rating'], axis=1)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df = df.dropna()\n",
    "```\n",
    "\n",
    "- Dropping columns that are not relevant to our analysis.\n",
    "- Dropping rows with missing values using dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Columns\n",
    "\n",
    "# Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will handle both \"90 min\" and \"1 Season\" formats\n",
    "df['duration'] = df['duration'].str.extract('(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detecting Outliers\n",
    "Outliers can skew results, especially in calculations like mean and standard deviation. We will detect and handle outliers.\n",
    "\n",
    "```python \n",
    "# Detecting outliers using quantiles\n",
    "quantile_95 = df['duration'].quantile(0.95)\n",
    "print(f\"95th percentile value of 'duration': {quantile_95}\")\n",
    "\n",
    "```\n",
    "\n",
    "This code identifies values above the 95th percentile for the 'duration' column. You can repeat this process for other numeric columns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile value of 'duration': 139.0\n"
     ]
    }
   ],
   "source": [
    "# Detecting outliers using quantiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Duplicates\n",
    "To ensure data integrity, we'll remove duplicate rows from the dataset.\n",
    "\n",
    "```python \n",
    "# Dropping duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "```\n",
    "You can also drop duplicates based on specific columns if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Renaming Columns\n",
    "For easier reference, we'll rename certain columns.\n",
    "\n",
    "```python\n",
    "# Renaming columns\n",
    "df = df.rename(columns={\n",
    "    \"show_id\": \"ID\", \n",
    "    \"type\": \"Type\", \n",
    "    \"title\": \"Title\", \n",
    "    \"duration\": \"Duration\",\n",
    "    \"listed_in\": \"Category\", \n",
    "    \"description\": \"Description\"\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inconsistent Data\n",
    "Data inconsistency issues can arise from typos, inconsistent formats, etc. We'll standardize data formats and convert data types as needed.\n",
    "\n",
    "```python\n",
    "# Standardizing text data to lowercase and removing spaces\n",
    "df['Title'] = df['Title'].str.lower().str.strip()\n",
    "\n",
    "# Converting data types (e.g., 'release_year' to datetime)\n",
    "df['release_year'] = pd.to_datetime(df['release_year'], format='%Y', errors='coerce')\n",
    "df['Type'] = df['Type'].astype('category')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing text data to lowercase and removing spaces\n",
    "\n",
    "# Converting Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "We can create new features from existing data, such as extracting the release year from the date.\n",
    "\n",
    "```python\n",
    "# Extracting year and month from 'release_year'\n",
    "df['release_year'] = df['release_year'].dt.year\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year and month from 'release_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection\n",
    "Decide which features are most relevant to keep in the dataset for further analysis.\n",
    "\n",
    "```python \n",
    "# Dropping irrelevant features\n",
    "df = df.drop(['ID', 'Description'], axis=1)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Transformation\n",
    "We'll explore normalization, standardization, and log transformations to handle data distributions.\n",
    "\n",
    "```python \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df['duration_normalized'] = scaler.fit_transform(df[['Duration']])\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "df['duration_standardized'] = scaler.fit_transform(df[['Duration']])\n",
    "\n",
    "# Log Transformation to handle skewness\n",
    "df['duration_log'] = np.log(df['Duration'] + 1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "\n",
    "# Standardization\n",
    "\n",
    "# Log Transformation to handle skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Encoding Categorical Variables\n",
    "We'll use one-hot encoding for non-ordinal categorical variables and label encoding for ordinal data.\n",
    "\n",
    "```python \n",
    "# One-hot encoding for 'Type'\n",
    "df = pd.get_dummies(df, columns=['Type'])\n",
    "\n",
    "# Label encoding for 'Category' (if it had ordinal values)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Category_encoded'] = le.fit_transform(df['Category'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for 'Type'\n",
    "\n",
    "# Label encoding for category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Binning\n",
    "Binning can help convert continuous data into discrete categories.\n",
    "```python \n",
    "# Binning duration into categories\n",
    "df['duration_bin'] = pd.cut(df['Duration'], bins=[0, 30, 60, 90, 120], labels=['Short', 'Medium', 'Long', 'Extended'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning duration into categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhdac.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
